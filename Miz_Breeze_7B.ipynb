{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mar 3rd: transformers 4.34.1 -> 4.36.0, flash_attn added\n",
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.36.0 trl==0.4.7 flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, MixtralForCausalLM\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = MixtralForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL)\n",
    "# Read prompt data from excel file\n",
    "import pandas\n",
    "\n",
    "df = pandas.read_excel(\"起訴狀案例測試.xlsx\")\n",
    "test_prompt = df.loc[9, 'prompt-claude3-Opus']\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = f\"\"\"請以中華民國律師的身分產生一份中華民國起訴書中請求「慰撫金」的內容，事件資料如下：\n",
    "        原告: 陳○麗, 原告身分證明類型: 國民身份證, 原告證號:O778541223, 原告年齡: 22, 原告性別: 女, 原告現住地: 新北市中和區國凱街32之3號3樓, 原告電子郵件: a54@gmail.com,\n",
    "        被告: 林O廉, 被告身分證明類型: 國民身份證, 被告證號:P224155693, 被告年齡: 30, 被告性別: 男, 被告現住地: 新北市板橋區龍泉街108巷9號2樓, 被告電子郵件: ac45@gmail.com,\n",
    "        事件內容:\n",
    "        醫療期間(天)：100\n",
    "        日常生活影響：車禍致使原告腳部受傷，無法行走，生活難以自理\n",
    "        身心健康影響：原告終日無神 經精神科鑑定為...\n",
    "        家庭影響：事件導致被害人親屬離世\n",
    "        離世被害人親屬：陳○○（父）\n",
    "        事件經過描述：緣原告於民國（下同） 87年 8月 9日時遭被告駕駛車號QQQ-8888 汽車碰撞，致原告身體受傷及原告之親屬死亡。...\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"你的中文能力如何?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"我可以進行流暢的中文對話，請儘管發問!\"},\n",
    "    #{\"role\": \"user\", \"content\": \"你知道一份法律起訴書是什麼樣的嗎？\"},\n",
    "    {\"role\": \"user\", \"content\": test_prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(input_ids, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL)\n",
    "# Preprocess & use dataset from drive\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "from datasets import load_dataset\n",
    "\n",
    "finetune_data = []\n",
    "\n",
    "file_path = 'dataset_for_training/indictment_json/'\n",
    "for file_name in [file for file in os.listdir(file_path) if file.endswith('.json')]:\n",
    "    with open(os.path.join(file_path, file_name), \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "        finetune_data.append({\"prompt\":str(data),\"response\":\"這是一份中華民國的起訴書\"})\n",
    "\n",
    "# Shuffle the combined data to ensure randomness\n",
    "shuffle(finetune_data)\n",
    "\n",
    "# Create a converted dataset with train_test split\n",
    "# Define the split sizes (e.g., \"80% train + 20% test\" would be 0.8)\n",
    "split_index = int(0.8 * len(finetune_data))\n",
    "\n",
    "train_data = finetune_data[:split_index]\n",
    "test_data = finetune_data[split_index:]\n",
    "\n",
    "with open('train_dataset.jsonl', 'w') as train_file:\n",
    "    for item in train_data:\n",
    "        train_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('test_dataset.jsonl', 'w') as test_file:\n",
    "    for item in test_data:\n",
    "        test_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "######\n",
    "\n",
    "# Read dataset\n",
    "train_dataset = load_dataset('json', data_files='train_dataset.jsonl', split=\"train\")  # 從JSON文件中載入訓練數據集\n",
    "valid_dataset = load_dataset('json', data_files='test_dataset.jsonl', split=\"train\")  # 從JSON文件中載入驗證數據集\n",
    "\n",
    "# preprocess dataset by text-pairing prompt and response\n",
    "train_dataset = train_dataset.map(lambda examples: {'text': [prompt + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n",
    "valid_dataset = valid_dataset.map(lambda examples: {'text': [prompt + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n",
    "\n",
    "\n",
    "######\n",
    "\n",
    "for i in train_dataset:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaTokenizer, MixtralForCausalLM, # added on Feb 29th\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the HuggingFace hub\n",
    "###model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "###model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "###model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n",
    "###model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "model_name = \"MediaTek-Research/Breeze-7B-Instruct-v1_0\"\n",
    "\n",
    "# The instruction dataset to use (Fetch from HuggingFace)\n",
    "# Disable if using dataset from drive\n",
    "###dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "###train_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"Llama-2-7b-chat-finetune\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 2\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 4\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = True\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 2048\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = MixtralForCausalLM.from_pretrained( # Feb 29th: AutomodelForCausalLM -> MixtralForCausalLM\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    use_flash_attention_2 = True # added on Feb 29th\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, trust_remote_code=True) # Feb 29th: AutoTokenizer -> LlamaTokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    target_modules=[\n",
    "      \"q_proj\",\n",
    "      \"k_proj\",\n",
    "      \"v_proj\",\n",
    "      \"o_proj\",\n",
    "      \"gate_proj\",\n",
    "      \"up_proj\",\n",
    "      \"down_proj\",\n",
    "      \"lm_head\",\n",
    "      ], # March 7th\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    #test_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL)\n",
    "# Visualize training\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir results/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"請使用中文描述中華民國的起訴書之結構\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
    "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = MixtralForCausalLM.from_pretrained( # Feb 29th: AutomodelForCausalLM -> MixtralForCausalLM\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, trust_remote_code=True) # Feb 29th: AutoTokenizer -> LlamaTokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL)\n",
    "# Saving the fine-tuned model to Local file\n",
    "model_save_path = '/app/my_model_directory'\n",
    "\n",
    "# Create the save directory if it does not exist\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "# Save the model and the tokenizer\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL)\n",
    "# Load the saved model from drive\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, MixtralForCausalLM, pipeline # Feb 29th\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "model_path = \"/content/drive/My Drive/my_model_directory\"  # 更改為您儲存路徑的模型\n",
    "\n",
    "model = MixtralForCausalLM.from_pretrained(model_path, # Feb 29th\n",
    "                         device_map=\"cuda\",\n",
    "                         offload_folder=\"offload\",\n",
    "                         torch_dtype=torch.float16)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path) # Feb 29th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL)\n",
    "# Upload the finetuned model to HuggingFace #Default\n",
    "# Input interaction is known to not work well in Jupyter\n",
    "\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "!huggingface-cli login\n",
    "\n",
    "model.push_to_hub(\"Llamarider222/Llama-2-7b-chat-hf\", check_pr=True)\n",
    "\n",
    "tokenizer.push_to_hub(\"Llamarider222/Llama-2-7b-chat-hf\",check_pr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the finetuned model to HuggingFace\n",
    "# Workaround 01\n",
    "\n",
    "import os\n",
    "\n",
    "# Replace YOUR_TOKEN with your actual Hugging Face API token.\n",
    "os.environ['HF_HOME'] = '/root/.cache/huggingface'  # Set cache directory if needed\n",
    "os.environ['HUGGINGFACE_HUB_TOKEN'] = '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'\n",
    "\n",
    "# Now you can push to the hub\n",
    "model.push_to_hub(\"Llamarider222/llawma-2-7b-chat-hf\", use_auth_token=True)\n",
    "tokenizer.push_to_hub(\"Llamarider222/llawma-2-7b-chat-hf\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the finetuned model to HuggingFace\n",
    "# Workaround 02\n",
    "\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# Replace YOUR_TOKEN_HERE with your actual token\n",
    "HfFolder.save_token('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')\n",
    "\n",
    "# Now you can push to the hub\n",
    "model.push_to_hub(\"Llamarider222/llama-2-7b-chat-hf\", use_auth_token=True)\n",
    "tokenizer.push_to_hub(\"Llamarider222/llama-2-7b-chat-hf\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the finetuned model to HuggingFace\n",
    "# Workaround 03\n",
    "\n",
    "!pip install --upgrade transformers\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    #model_path,\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "model.push_to_hub(\"Llamarider222/llama-2-7b-chat-hf\", use_auth_token=True)\n",
    "tokenizer.push_to_hub(\"Llamarider222/llama-2-7b-chat-hf\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL)\n",
    "# load the public model from huggingface\n",
    "\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "!pip install transformers accelerate\n",
    "!pip install sentencepiece\n",
    "\n",
    "import transformers\n",
    "from transformers import LlamaTokenizer, MixtralForCausalLM, AutoModel, AutoTokenizer # Feb 29th\n",
    "import torch\n",
    "\n",
    "model = \"Llamarider222/Llama-2-7b-chat-hf\"\n",
    "prompt = \"請描述中華民國的起訴書的內容,包含'indictNO','indictment','org','date','type','reason','relatedIssues'等標籤。\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    f'[INST] {prompt} [/INST]',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test the Finetuned Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nth test-run\n",
    "# postFT\n",
    "\n",
    "prompt=\"請描述中華民國起訴書的內容，包含'indictNO','indictment','org','date','type','reason','relatedIssues'等標籤。\"\n",
    "\n",
    "gen = pipeline('text-generation', model=model, max_new_tokens= 200, tokenizer=tokenizer)\n",
    "result = gen(prompt)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nth test-run\n",
    "# preFT\n",
    "\n",
    "prompt=\"請描述中華民國起訴書的內容，包含'indictNO','indictment','org','date','type','reason','relatedIssues'等標籤。\"\n",
    "\n",
    "gen = pipeline('text-generation', model=model, max_new_tokens= 2048, tokenizer=tokenizer)\n",
    "result = gen(prompt)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradio UI__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHAT UI v0.0.4(latest)\n",
    "import os\n",
    "import gradio as gr\n",
    "from matplotlib.rcsetup import validate_fontsize_None\n",
    "from pickle import FALSE\n",
    "\n",
    "#帳密\n",
    "acpw_dict = [{'ac': '2665', 'pw': '89944'}, {'ac': '2675', 'pw': '898s8s4'}, {'ac': 'lee', 'pw': '12345'}, {'ac': 'test', 'pw': '12345'}]\n",
    "pw_show = False\n",
    "\n",
    "with gr.Blocks() as acpw:\n",
    "    #登入介面\n",
    "    gr.Markdown(\"請輸入帳密\")\n",
    "    ac = gr.Textbox(label=\"帳號\")\n",
    "    pw = gr.Textbox(label=\"密碼\", type=\"password\")\n",
    "    sysoutput = gr.Textbox(label=\"系統訊息\", interactive=False)\n",
    "    logi = gr.Button(\"登入\")\n",
    "    regi = gr.Button(\"註冊\")\n",
    "    logi_regi = gr.Button(\"註冊並登入\")\n",
    "    default_murder_input = gr.Button(\"預設資料(謀殺案)\")\n",
    "    default_burglary_input = gr.Button(\"預設資料(竊盜案)\")\n",
    "    #showing_pw = gr.Button(\"密碼顯示\")\n",
    "\n",
    "    #聊天介面&輸入基本資料介面(登入成功才顯示)\n",
    "    #輸入原告資料\n",
    "    with gr.Column(visible=False) as plaintiff_ui:\n",
    "      # Add 5 input fields for personal data\n",
    "      Plaintiff_name_input = gr.Textbox(label=\"原告\")\n",
    "      Plaintiff_ID_type_input = gr.Dropdown(label=\"身份證明文件\", choices=[\"國民身份證\", \"護照\", \"居留證\", \"工作證\", \"營利事業登記\"])\n",
    "      Plaintiff_ID_input = gr.Textbox(label=\"證號\")\n",
    "      Plaintiff_age_input = gr.Textbox(label=\"年齡\")\n",
    "      Plaintiff_birth_input = gr.Textbox(label=\"生日\")\n",
    "      Plaintiff_gender_input = gr.Dropdown(label=\"性別\", choices=[\"男\", \"女\", \"其他\"])\n",
    "      Plaintiff_location_input = gr.Textbox(label=\"現住地\")\n",
    "      Plaintiff_telephone_input = gr.Textbox(label=\"電話\")\n",
    "      Plaintiff_email_input = gr.Textbox(label=\"電子郵件\")\n",
    "\n",
    "    #輸入被告資料\n",
    "    with gr.Column(visible=False) as defendant_ui:\n",
    "      Defendant_name_input = gr.Textbox(label=\"被告\")\n",
    "      Defendant_ID_type_input = gr.Dropdown(label=\"身份證明文件\", choices=[\"國民身份證\", \"護照\", \"居留證\", \"工作證\", \"營利事業登記\"])\n",
    "      Defendant_ID_input = gr.Textbox(label=\"證號\")\n",
    "      Defendant_age_input = gr.Textbox(label=\"年齡\")\n",
    "      Defendant_birth_input = gr.Textbox(label=\"生日\")\n",
    "      Defendant_gender_input = gr.Dropdown(label=\"性別\", choices=[\"男\", \"女\", \"其他\"])\n",
    "      Defendant_location_input = gr.Textbox(label=\"現住地\")\n",
    "      Defendant_telephone_input = gr.Textbox(label=\"電話\")\n",
    "      Defendant_email_input = gr.Textbox(label=\"電子郵件\")\n",
    "\n",
    "    #輸入事發經過\n",
    "    with gr.Column(visible=False) as detail_ui:\n",
    "      detail_input = gr.Textbox(label=\"事件描述：\")\n",
    "      # Button to combine data\n",
    "      combine_button = gr.Button(\"產生PROMPT\")\n",
    "      #combine_DEFAULT_button = gr.Button(\"產生預設PROMPT\")\n",
    "      # Chat Interface\n",
    "      chatbot = gr.Chatbot()\n",
    "      with gr.Row():\n",
    "        msg = gr.Textbox(\n",
    "            container=False,\n",
    "            show_label=False,\n",
    "            placeholder='Type a message...',\n",
    "            scale=10,\n",
    "        )\n",
    "        submit_button = gr.Button('Submit',variant='primary',scale=1,min_width=0)\n",
    "\n",
    "\n",
    "    def user(user_message, history):\n",
    "      return \"\", history + [[user_message, None]]\n",
    "\n",
    "    #整合資料\n",
    "    def combine_data(Plaintiff_name, Plaintiff_id_type, Plaintiff_id, Plaintiff_age, Plaintiff_gender, Plaintiff_location, Plaintiff_email, Defendant_name, Defendant_id_type, Defendant_id, Defendant_age, Defendant_gender, Defendant_location, Defendant_email, detail):\n",
    "      combinedtextdata = f\"\"\"請以中華民國律師的身分產生一份中華民國起訴書\n",
    "      原告: {Plaintiff_name}, 原告身分證明類型: {Plaintiff_id_type}, 原告證號:{Plaintiff_id}, 原告年齡: {Plaintiff_age}, 原告性別: {Plaintiff_gender}, 原告現住地: {Plaintiff_location}, 原告電子郵件: {Plaintiff_email},\n",
    "      被告: {Defendant_name}, 被告身分證明類型: {Defendant_id_type}, 被告證號:{Defendant_id}, 被告年齡: {Defendant_age}, 被告性別: {Defendant_gender}, 被告現住地: {Defendant_location}, 被告電子郵件: {Defendant_email},\n",
    "      事件內容: {detail}\"\"\"\n",
    "      return combinedtextdata\n",
    "    #對話系統\n",
    "    def bot(history):\n",
    "      gen = pipeline('text-generation', model=model, max_new_tokens= 256, tokenizer=tokenizer)\n",
    "      result = gen(history[-1][0])\n",
    "      bot_message = result[0]['generated_text']\n",
    "      history[-1][1] = \"\"\n",
    "      for character in bot_message:\n",
    "          history[-1][1] += character\n",
    "          yield history\n",
    "\n",
    "    #謀殺案 填入資料函式\n",
    "    def default_murder_input_change_tetbox():\n",
    "      return gr.update(value=\"陳○麗\"),gr.update(value=\"國民身份證\"),gr.update(value=\"O778541223\"),gr.update(value=\"22\"),gr.update(value=\"新北市中和區國凱街32之3號3樓\"),gr.update(value=\"a54@gmail.com\"),gr.update(value=\"林學廉\"),gr.update(value=\"國民身份證\"),gr.update(value=\"P224155693\"),gr.update(value=\"30\"),gr.update(value=\"新北市板橋區龍泉街108巷9號2樓\"),gr.update(value=\"ac45@gmail.com\"),gr.update(value=\"林學廉與陳○麗原為同居之男女朋友，屬家庭暴力防治法所稱之家庭成員，因感情糾紛。於106年9月27日0時16分，在屋內林學廉因感情之事與陳○麗起口角，林學廉心生不滿，走出屋外持事先預備之西瓜刀，插在腰間走入屋內，見陳○麗坐在床上直接過去砍她。\")\n",
    "      #gr.update(value=\"女\"),\n",
    "      #gr.update(value=\"男\"),\n",
    "\n",
    "    #竊盜案 填入資料函式\n",
    "    def default_burglary_input_change_tetbox():\n",
    "      return gr.update(value=\"許洋偉\"),gr.update(value=\"國民身份證\"),gr.update(value=\"O778541223\"),gr.update(value=\"22\"),gr.update(value=\"新北市中和區國凱街32之3號3樓\"),gr.update(value=\"a54@gmail.com\"),gr.update(value=\"黃奕凱\"),gr.update(value=\"國民身份證\"),gr.update(value=\"P224155693\"),gr.update(value=\"30\"),gr.update(value=\"新北市板橋區龍泉街108巷9號2樓\"),gr.update(value=\"ac45@gmail.com\"),gr.update(value = \"黃奕凱於107年6月5日凌晨1時48分左右，駕駛 自用小客車，行經國凱街旁邊空地，見四周無人，竟意圖為自己不法之所有，基於竊盜之犯意，以自備吸油工具插入許洋偉停放於該處之自用小貨車油箱內，將新臺幣300元之95無鉛汽油10公升抽出置入自備油桶，再加入其駕駛之自用小客車油箱內，旋即往國凱街方向逃逸。許洋偉發覺其車輛油箱蓋遭開啟且油表之油量減少，發覺遭竊，報警處理。\" )\n",
    "      #gr.update(value=\"女\"),\n",
    "      #gr.update(value=\"男\"),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #AI對話方法(留存備用)\n",
    "    \"\"\"\n",
    "    def chat_with_ai(message, history):\n",
    "      gen = pipeline('text-generation', model=model, max_new_tokens= 256, tokenizer=tokenizer)\n",
    "      result = gen(message)\n",
    "      yield result[0]['generated_text']\n",
    "    \"\"\"\n",
    "    #Combine data\n",
    "    combine_button.click(fn=combine_data, inputs=[Plaintiff_name_input,\n",
    "                    Plaintiff_ID_type_input,\n",
    "                    Plaintiff_ID_input,\n",
    "                    Plaintiff_age_input,\n",
    "                    Plaintiff_gender_input,\n",
    "                    Plaintiff_location_input,\n",
    "                    Plaintiff_email_input,\n",
    "                    Defendant_name_input,\n",
    "                    Defendant_ID_type_input,\n",
    "                    Defendant_ID_input,\n",
    "                    Defendant_age_input,\n",
    "                    Defendant_gender_input,\n",
    "                    Defendant_location_input,\n",
    "                    Defendant_email_input,\n",
    "                    detail_input],\n",
    "                    outputs=[msg] )\n",
    "\n",
    "\n",
    "\n",
    "    #謀殺案 預設填入資料\n",
    "    default_murder_input.click(default_murder_input_change_tetbox , inputs=[],\n",
    "\n",
    "                    outputs=[Plaintiff_name_input,Plaintiff_ID_type_input,Plaintiff_ID_input,Plaintiff_age_input,Plaintiff_location_input,Plaintiff_email_input, Defendant_name_input,Defendant_ID_type_input,Defendant_ID_input,Defendant_age_input,Defendant_location_input,Defendant_email_input,detail_input\n",
    "\n",
    "\n",
    "                    #Plaintiff_gender_input,\n",
    "\n",
    "                    #Defendant_gender_input,\n",
    "                    ])\n",
    "    #竊盜案 預設填入資料\n",
    "    default_burglary_input.click(default_burglary_input_change_tetbox , inputs=[],\n",
    "\n",
    "                    outputs=[Plaintiff_name_input,Plaintiff_ID_type_input,Plaintiff_ID_input,Plaintiff_age_input,Plaintiff_location_input,Plaintiff_email_input, Defendant_name_input,Defendant_ID_type_input,Defendant_ID_input,Defendant_age_input,Defendant_location_input,Defendant_email_input,detail_input\n",
    "\n",
    "\n",
    "                    #Plaintiff_gender_input,\n",
    "\n",
    "                    #Defendant_gender_input,\n",
    "                    ])\n",
    "\n",
    "    \"\"\"\n",
    "    #預設內容\n",
    "    combine_DEFAULT_button.click(fn=combine_data, inputs=[\n",
    "                    \"陳○麗\",\n",
    "                    \"國民身份證\",\n",
    "                    \"O778541223\",\n",
    "                    \"22\",\n",
    "                    \"女\",\n",
    "                    \"新北市中和區國凱街32之3號3樓\",\n",
    "                    \"a5541105114@gmail.com\",\n",
    "                    \"林學廉\",\n",
    "                    \"國民身份證\",\n",
    "                    \"P224155693\",\n",
    "                    \"30\",\n",
    "                    \"男\",\n",
    "                    \"新北市板橋區龍泉街108巷9號2樓\",\n",
    "                    \"c885445@gmail.com\",\n",
    "                    \"林學廉與陳○麗原為同居之男女朋友，屬家庭暴力防治法所稱之家庭成員，前因感情糾紛。嗣於106年9月27日0時16分許，在上址屋內，林學廉酒後因 感情之事與陳○麗起口角，林學廉因而心生不滿，竟走出屋外持其事先預備之西瓜刀，插在腰間走入屋內，見陳○麗坐在床上直接過去砍她。\"],\n",
    "                    outputs=[msg] )\n",
    "    \"\"\"\n",
    "    #Submit\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(fn=bot, inputs=chatbot, outputs=chatbot)\n",
    "    submit_button.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(fn=bot, inputs=chatbot, outputs=chatbot)\n",
    "\n",
    "    #密碼輸入長度\n",
    "    def welcome(pw):\n",
    "      pw_leng = len(pw)\n",
    "      return \"Password Length:\" + str(pw_leng)\n",
    "\n",
    "    #登入判斷\n",
    "    def confirm_ac_pw(ac, pw):\n",
    "      for i in acpw_dict:\n",
    "          if i['ac'] == ac:\n",
    "              if i['pw'] == pw:\n",
    "                return {sysoutput:\"登入成功\", plaintiff_ui:gr.Column(visible=True), defendant_ui:gr.Column(visible=True), detail_ui:gr.Column(visible=True)}\n",
    "          else:\n",
    "              print()\n",
    "      return {sysoutput:\"登入失敗\"}\n",
    "    #註冊\n",
    "    def register_ac_pw(ac, pw):\n",
    "      acpw_dict.append({'ac': ac, 'pw': pw})\n",
    "      return \"註冊成功\"\n",
    "\n",
    "\n",
    "    #註冊並登入(測試用)\n",
    "    def logi_register_ac_pw(ac, pw):\n",
    "      acpw_dict.append({'ac': ac, 'pw': pw})\n",
    "      return {sysoutput:\"註冊且登入成功\", plaintiff_ui:gr.Column(visible=True), defendant_ui:gr.Column(visible=True), detail_ui:gr.Column(visible=True)}\n",
    "\n",
    "    #登入介面功能\n",
    "    logi.click(confirm_ac_pw, [ac, pw], [sysoutput, plaintiff_ui, defendant_ui, detail_ui])\n",
    "    regi.click(register_ac_pw, [ac, pw], [sysoutput])\n",
    "    logi_regi.click(logi_register_ac_pw, [ac, pw], [sysoutput, plaintiff_ui, defendant_ui, detail_ui])\n",
    "    pw.change(welcome, pw , sysoutput)\n",
    "\n",
    "\n",
    "acpw.queue()\n",
    "if __name__ == \"__main__\":\n",
    "  #啟動UI\n",
    "  acpw.launch(show_api=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cpu\n",
      "False\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 20 20:03:35 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.79                 Driver Version: 531.79       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090       WDDM | 00000000:01:00.0 Off |                  Off |\n",
      "|  0%   40C    P8               14W / 450W|      0MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check the GPU in-use & specs\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\miz\\anaconda3\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: torch in c:\\users\\miz\\anaconda3\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\miz\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\miz\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\miz\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\miz\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\miz\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\miz\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\miz\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\miz\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\miz\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install -y git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: packaging in c:\\users\\miz\\anaconda3\\lib\\site-packages (23.1)\n",
      "Collecting ninja\n",
      "  Obtaining dependency information for ninja from https://files.pythonhosted.org/packages/b6/2f/a3bc50fa63fc4fe9348e15b53dc8c87febfd4e0c660fcf250c4b19a3aa3b/ninja-1.11.1.1-py2.py3-none-win_amd64.whl.metadata\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-win_amd64.whl.metadata (5.4 kB)\n",
      "Downloading ninja-1.11.1.1-py2.py3-none-win_amd64.whl (312 kB)\n",
      "   ---------------------------------------- 0.0/313.0 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 71.7/313.0 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 313.0/313.0 kB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: ninja\n",
      "Successfully installed ninja-1.11.1.1\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.5.6.tar.gz (2.5 MB)\n",
      "     ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/2.5 MB 991.0 kB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 0.3/2.5 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.7/2.5 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 1.1/2.5 MB 6.6 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.5/2.5 MB 6.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.7/2.5 MB 6.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 2.0/2.5 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 2.3/2.5 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.5/2.5 MB 6.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [22 lines of output]\n",
      "      fatal: not a git repository (or any of the parent directories): .git\n",
      "      \n",
      "      \n",
      "      torch.__version__  = 2.2.1+cpu\n",
      "      \n",
      "      \n",
      "      C:\\Users\\Miz\\AppData\\Local\\Temp\\pip-install-28cma6uk\\flash-attn_c792f56c13704d4d81c3d9d79928eb37\\setup.py:78: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\n",
      "        warnings.warn(\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\Miz\\AppData\\Local\\Temp\\pip-install-28cma6uk\\flash-attn_c792f56c13704d4d81c3d9d79928eb37\\setup.py\", line 133, in <module>\n",
      "          CUDAExtension(\n",
      "        File \"C:\\Users\\Miz\\anaconda3\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1074, in CUDAExtension\n",
      "          library_dirs += library_paths(cuda=True)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Miz\\anaconda3\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1208, in library_paths\n",
      "          paths.append(_join_cuda_home(lib_dir))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Miz\\anaconda3\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 2407, in _join_cuda_home\n",
      "          raise OSError('CUDA_HOME environment variable is not set. '\n",
      "      OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install packaging ninja\n",
    "!pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Instruction Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"MediaTek-Research/Breeze-7B-Instruct-v1_0\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"flash_attention_2\" # optional\n",
    ")\n",
    "\n",
    "# Basemodel\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"MediaTek-Research/Breeze-7B-Base-v1_0\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"flash_attention_2\" # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MediaTek-Research/Breeze-7B-Instruct-v1_0\")\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"你好，請問你可以完成什麼任務？\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"你好，我可以幫助您解決各種問題、提供資訊和協助您完成許多不同的任務。例如：回答技術問題、提供建議、翻譯文字、尋找資料或協助您安排行程等。請告訴我如何能幫助您。\"},\n",
    "  {\"role\": \"user\", \"content\": \"太棒了！\"},\n",
    "]\n",
    "tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(tokenizer.apply_chat_template(chat, return_tensors=\"pt\"),\n",
    "                         # adjust below parameters if necessary \n",
    "                         max_new_tokens=128,\n",
    "                         top_p=0.01,\n",
    "                         top_k=85,\n",
    "                         repetition_penalty=1.1,\n",
    "                         temperature=0.01)\n",
    "                         \n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL)\n",
    "# Read prompt data from excel file\n",
    "import pandas\n",
    "\n",
    "df = pandas.read_excel(\"gen_sample.xlsx\")\n",
    "test_prompt = df.loc[9, 'prompt-claude3-Opus']\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference based of the excel file\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"你的中文能力如何?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"我可以進行流暢的中文對話，請儘管發問!\"},\n",
    "    {\"role\": \"user\", \"content\": \"你知道一份法律起訴書是什麼樣的嗎？\"}\n",
    "    {\"role\": \"user\", \"content\": test_prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(input_ids, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
