{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May 16th: Transformer 4.37.0 via pip + AutoGPTQ 0.8.0dev0 from source (git clone) works on Colab\n",
    "!pip install transformers==4.37.0\n",
    "###!pip3 install git+https://github.com/huggingface/transformers\n",
    "\n",
    "!git clone https://github.com/AutoGPTQ/AutoGPTQ.git\n",
    "%cd AutoGPTQ\n",
    "!pip install -e .\n",
    "\n",
    "# Upgrade relevant package\n",
    "\n",
    "!pip install --upgrade trl peft accelerate bitsandbytes datasets optimum -q\n",
    "\n",
    "# Verify the versions\n",
    "import transformers\n",
    "import auto_gptq\n",
    "\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"AutoGPTQ version:\", auto_gptq.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, MixtralForCausalLM, DataCollatorForLanguageModeling, BitsAndBytesConfig, GPTQConfig\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, prepare_model_for_kbit_training, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL) Load dataset from HuggingFace\n",
    "\n",
    "dataset = load_dataset('gbharti/finance-alpaca')\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "# Further split the train dataset into train and validation sets\n",
    "train_val_split = train_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_val_split['train']\n",
    "eval_dataset = train_val_split['test']\n",
    "\n",
    "pretrained_model_name_or_path = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL) Read prompt data from excel file\n",
    "\n",
    "import os\n",
    "import pandas\n",
    "import json\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "finetune_prompt_train = []  # Initialize as a list\n",
    "finetune_prompt_test = []\n",
    "df = pandas.read_excel(\"/content/gen_sample_20.xlsx\")\n",
    "'''for i in range(19):\n",
    "    prompt = df.loc[i, '(模擬律師output)輸入內容']\n",
    "    finetune_answer = df.loc[i, '(起訴狀answer)H3']\n",
    "    finetune_temp = f\"\"\"<s>[INST]{prompt}[/INST]{finetune_answer}</s>\"\"\"\n",
    "    if i < 16:\n",
    "      finetune_prompt_train.append(finetune_temp)  # Append to the list\n",
    "    else:\n",
    "      finetune_prompt_test.append(finetune_temp)  # Append to the list\n",
    "print(finetune_prompt_train[0])\n",
    "print(finetune_prompt_test[0])'''\n",
    "\n",
    "def create_text_row(instruction, output):\n",
    "  #暫時捨棄了input以減少token數\n",
    "    text_row = f\"\"\"<s>[INST] {instruction} [/INST] \\n {output} </s>\"\"\"\n",
    "    return text_row\n",
    "\n",
    "# interate over all the rows formate the dataset and store it in a jsonl file\n",
    "def process_jsonl_file(output_file_path):\n",
    "    with open(output_file_path, \"w\") as output_jsonl_file:\n",
    "        for i in range(20):\n",
    "            instruction = df.loc[i, '(模擬律師output)輸入內容']\n",
    "            output = df.loc[i, '(起訴狀answer)H3']\n",
    "            json_object = {\n",
    "                \"text\": create_text_row(instruction, output),\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": \"\", # 暫時捨棄了input以減少token數\n",
    "                \"output\": output\n",
    "            }\n",
    "            output_jsonl_file.write(json.dumps(json_object, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Provide the path where you want to save the formatted dataset\n",
    "process_jsonl_file(\"./train_dataset.jsonl\")\n",
    "dataset = load_dataset('json', data_files='./train_dataset.jsonl' , split='train')\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "# Further split the train dataset into train and validation sets\n",
    "train_val_split = train_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_val_split['train']\n",
    "eval_dataset = train_val_split['test']\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "###pretrained_model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "pretrained_model_name_or_path = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
    "revision=\"gptq-3bit--1g-actorder_True\"\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def format_input_data_to_build_model_prompt(data_point):\n",
    "        instruction = str(data_point['instruction'])\n",
    "        input_query = str(data_point['input'])\n",
    "        response = str(data_point['output'])\n",
    "\n",
    "        if len(input_query.strip()) == 0:\n",
    "            full_prompt_for_model = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction} \\n\\n### Input:\\n{input_query}\\n\\n### Response:\\n{response}\"\"\"\n",
    "\n",
    "        else:\n",
    "            full_prompt_for_model = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\"\"\n",
    "        return tokenize(full_prompt_for_model)\n",
    "\n",
    "def build_qlora_model(\n",
    "    ###pretrained_model_name_or_path: str = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\",\n",
    "    pretrained_model_name_or_path: str = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\",\n",
    "    gradient_checkpointing: bool = True,\n",
    "    cache_dir: Optional[Path] = None,\n",
    ") -> Tuple[MixtralForCausalLM, AutoTokenizer, PeftConfig]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pretrained_model_name_or_path (str): The name or path of the pretrained model to use.\n",
    "        gradient_checkpointing (bool): Whether to use gradient checkpointing or not.\n",
    "        cache_dir (Optional[Path]): The directory to cache the model in.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[MixtralForCausalLM, AutoTokenizer]: A tuple containing the built model and tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Disable bnb_config if using any GPTQ model (Can't quantize an already quantized model)\n",
    "\n",
    "    # bnb_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    #     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    # )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        padding_side=\"left\",\n",
    "        add_eos_token=True,\n",
    "        add_bos_token=True,\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    quantization_config_loading = GPTQConfig(bits=3, use_exllama=False, tokenizer=tokenizer)\n",
    "\n",
    "    # Disable quantization_config param if using GPTQ models\n",
    "\n",
    "    model = MixtralForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        # quantization_config=bnb_config,\n",
    "        quantization_config=quantization_config_loading,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=str(cache_dir) if cache_dir else None,\n",
    "    )\n",
    "\n",
    "    # Disable tensor parallelism\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    if gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.config.use_cache = (\n",
    "            False  # Gradient checkpointing is not compatible with caching.\n",
    "        )\n",
    "    else:\n",
    "        model.gradient_checkpointing_disable()\n",
    "        model.config.use_cache = True  # It is good practice to enable caching when using the model for inference.\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = build_qlora_model(pretrained_model_name_or_path)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Output format:\n",
    "# trainable params: 6815744 || all params: 269225984 || trainable%: 2.5316070532033046\n",
    "\n",
    "# Apply the accelerator. (optional)\n",
    "model = accelerator.prepare_model(model)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(format_input_data_to_build_model_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(format_input_data_to_build_model_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabs a single data point from our testset to see how the base model does on it.\n",
    "\n",
    "print(\"Instruction Sentence: \" + test_dataset[1]['instruction'])\n",
    "print(\"Output: \" + test_dataset[1]['output'] + \"\\n\")\n",
    "\n",
    "# Prompt's template for the Instruct model is defined as follows:\n",
    "#<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"Given an instruction sentence construct the output.\n",
    "\n",
    "### Instruction sentence:\n",
    "Generate a sentence that describes the main idea behind a stock market crash.\n",
    "\n",
    "\n",
    "### Output\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the prepare_model_for_kbit_training method from PEFT.\n",
    "# Apply the accelerator. (prepare_model - Prepares a PyTorch model for training in any distributed setup.)\n",
    "model = accelerator.prepare_model(model)\n",
    "\n",
    "# Re-init the tokenizer so it doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "device = \"cuda\"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=128)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Torch, CUDA version\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "\n",
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "    print(\"Multiple CUDA detected, parallel model enabled.\")\n",
    "else:\n",
    "    print(\"A single CUDA detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable CUDA launch blocking for detailed error messages\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "project = \"Mixtral-alpaca-finance-finetune\"\n",
    "base_model_name = \"mixtral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=2, # reduced from 4 for testing\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5,\n",
    "        logging_steps=25,\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",                   # Directory for storing logs\n",
    "        save_strategy=\"steps\",                  # Save the model checkpoint every logging step\n",
    "        save_steps=50,                          # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\",            # Evaluate the model every logging step\n",
    "        eval_steps=50,                          # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                           # Perform evaluation at the end of training\n",
    "        # report_to=\"wandb\",                    # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\" # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-training Inference\n",
    "from transformers import pipeline, logging\n",
    "\n",
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"你的繁體中文能力如何? [/INST]我可以進行流暢的繁體中文對話，請儘管發問! [INST] 任務內容：從判決書的內容反推律師在撰寫起訴狀的時候的會使用的敘述。\\\n",
    "最下面會有判決書的內容，請你從判決書內容生成模擬律師如果要生成起訴狀的話，會有哪些必要的語句，盡量要口語化和簡單。\\\n",
    "我要改寫成三個大段落：\\\n",
    "一、 事故發生緣由\\\n",
    "二、 被告受傷情形\\\n",
    "三、 包含請求賠償的事實根據\\\n",
    "需要保留內容：重點事項要維持一樣的，包括如果原本在判決書中有條文的話，雖然不口語但是也要保留；如果是被告和原告的互動描述也要保留；受傷情形、事故發生緣由、包含請求賠償的事實根據保持原有的詳細程度；要保留相關的數字，像是住院長度，不能工作的長度，也都要保留；地址、姓名、車牌等訊息也要保留，不要自己新創內容；如果原文有引用判決的時候，必須要保留把判決的年度跟號碼寫出來。\\\n",
    "需要注意的內容：你是要模擬律師，所以主詞還是都要是'被告'或是'原告'，不要用'我'；'不要'把賠償金額加總起來或是計算，就單純保持原樣即可。\\\n",
    "內容要求：非常的口語化和簡單。\\\n",
    "以下為判決書內容： [/INST]請告訴我判決書內容以生成起訴狀 [INST] 一、事故發生緣由：\\\n",
    "被告在民國109年4月2日晚上7點40分左右，開著車牌號碼是0000-00的自用小客車，從臺北市文山區木新路3段的西邊往東邊開。開到木新路3段和興隆路4段的路口時，被告想要左轉到興隆路4段的北向車道。按照交通規則，被告應該要注意車子前面的狀況，隨時採取必要的安全措施，也要特別注意行人穿越道上有沒有行人。如果有行人要過馬路，不管有沒有交通指揮人員或號誌，被告都應該要停車讓行人先走。而且當時的天氣、路況和能見度都很好，被告沒有理由說他不能注意。但是被告就是疏忽大意，沒有發現原告正要從西邊往東邊走過行人穿越道，就這樣直接開過去，撞到了原告。原告因此受傷，傷勢包括頭部外傷、頸部拉傷和挫傷、右手肘和右手挫傷、下背部挫傷、頸部扭傷等等。\\\n",
    "\\\n",
    "二、被告受傷情形：\\\n",
    "原告在車禍發生當天，也就是109年4月2日，先被送到臺北市立萬芳醫院急診。後來又繼續在萬芳醫院門診追蹤治療，還去了吉辰中醫診所、春林復健科診所、臺北市立聯合醫院松德院區、中山醫療社團法人中山醫院、臺北榮民總醫院、尹書田醫療財團法人書田泌尿科眼科診所等醫院復健。原告為了生活上的需要，還花錢買了一些醫療用品。這些都有診斷證明書和醫療收據可以證明。原告請求被告賠償的醫療費用和增加生活上需要的費用，總共是13萬元，這是依照民法第193條第1項的規定。\\\n",
    "\\\n",
    "三、請求賠償的事實根據：\\\n",
    "在車禍發生之前，原告在基邑園藝工程有限公司上班，每個月薪水是45,000元。從車禍發生後到110年1月，原告已經有9個月不能工作，薪資損失本來應該有40萬5,000元，但是原告只請求20萬元而已。而且原告因為車禍受傷，身心都受到創傷，還得了重度憂鬱症和創傷後壓力症候群。車禍的後遺症讓原告沒辦法久站或久坐，到現在都還在休養當中，沒有辦法工作。此外，原告在這次車禍受傷以後，每次走在行人穿越道上都會擔心再次被車撞飛，精神一直處在緊張不安的狀態。而且到現在傷勢都還沒完全康復，要繼續門診追蹤和復健，嚴重影響了生活作息。原告的精神上受到很大的痛苦，心理壓力也很大，加上原告還要一個人扶養兩個還在讀書的小孩，所以原告請求被告賠償50萬元的精神慰撫金。\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=4096)\n",
    "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-training Inference\n",
    "from transformers import pipeline, logging\n",
    "\n",
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"\"\"你的繁體中文能力如何? [/INST]我可以進行流暢的繁體中文對話，請儘管發問! [INST] 你是一個中華民國的民法專長的律師，你主要是要撰寫交通事故的民事起訴狀，並且只針對慰撫金部分的案件作撰寫。慰撫金只會用到民法的內容。\n",
    "以下是民事起訴狀的結構：\n",
    "一、(這裡要填入詳細的事實緣由，要用法律用語)\n",
    "二、(這裡先'引用法條'，因為我們要生成的起訴狀是限定在有慰撫金的民事的交通事故，基本上都會用到第184條，請你注意。請先閱讀要改寫的內容，如果有請求看護費用，引用第193條會比較合理，如果沒有要賠償看護費用的話，就不須引用第193條。如果在2個以上的被告需要共同賠償的時候，大多會引用第185條第1項。接著會開始書寫連帶賠償。)\n",
    " |---(一)(這裡填入相關的賠償大項，以及說明，說明要很詳細。撰寫金額的時候統一使用'位數逗號的數字'；已支出費用跟未來發生的費用不可以寫在同一大項中，要分開敘述。)\n",
    " |         |---1.(這裡填入大項中的細項賠償，以及說明，說明要很詳細。撰寫金額的時候統一使用'位數逗號的數字'。)\n",
    " |                 |----(1)(這裡填入細項賠償中的說明，說明要很詳細。撰寫金額的時候統一使用'位數逗號的數字'。)\n",
    " |\n",
    " |---(六) (最後一個是要結尾這一大段的，要以'綜上所陳'開頭對這一段賠償做結尾；要把所有的賠償金額都相加起來，請務必要加總正確，撰寫金額的時候統一使用'位數逗號的數字'。)\n",
    "請將以下律師口語輸入的內容根據民事起訴狀的格式改寫出來。文章風格、格式和階層格式'務必'要與範本和我要的結構一樣；不要根據以下的格式；不要新創內容，完全依照以下我提供的事實做改寫：\n",
    "\"一、事故發生緣由:\n",
    "被告的機車駕照因為酒駕被註銷了,但是在民國110年8月18日晚上9點26分左右,他還是騎著車牌號碼是000-0000的重型機車,在新北市板橋區長江路1段由南往北往民生路的方向騎。雖然當時是晚上,但是路上有路燈、天氣晴朗、路面是乾的也沒有坑洞,視線也很好,被告應該要注意迴轉時要打方向燈,並且停下來看清楚有沒有車輛來往,可是被告卻疏忽了,沒有打方向燈就直接從路邊起駛要左轉。\n",
    "\n",
    "二、原告受傷情形:\n",
    "就在被告要左轉的時候,原告騎著車牌號碼是000-0000的重型機車從後面同向騎來,因為來不及閃,所以兩台機車就撞在一起。原告跟機車一起摔在地上,原告的左膝挫傷還合併十字韌帶撕裂性骨折,左肩也挫傷和擦傷。原告總共動了兩次手術,每次手術後都需要專人照顧一個月。受傷、開刀到術後休養加起來超過7、8個月不能工作。\n",
    "\n",
    "三、請求賠償的事實根據:\n",
    "原告請求被告賠償的項目包括:醫療費用171,170元、護具費用9,500元、就醫回診的交通費5,000元、看護費用90,000元、不能工作的薪資損失630,000元、維修受損機車的費用31,800元,以及精神慰撫金300,000元。原告在事故前受僱於力拔山工程有限公司當泥作師傅,每個月的薪水是105,000元。因為這起事故,原告的機車壞了需要修,身體也受傷需要手術和休養,不但身體感到疼痛,生活作息也受到很大影響,精神上也承受了相當大的痛苦。\" \"\"\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=4096)\n",
    "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "\n",
    "base_model = MixtralForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     return_dict=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map={\"\": 0},\n",
    "# )\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"mixtral-8x7b-finetune\"\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = f\"\"\"一、事故發生緣由：\n",
    "被告在民國109年4月2日晚上7點40分左右，開著車牌號碼是0000-00的自用小客車，從臺北市文山區木新路3段的西邊往東邊開。開到木新路3段和興隆路4段的路口時，被告想要左轉到興隆路4段的北向車道。按照交通規則，被告應該要注意車子前面的狀況，隨時採取必要的安全措施，也要特別注意行人穿越道上有沒有行人。如果有行人要過馬路，不管有沒有交通指揮人員或號誌，被告都應該要停車讓行人先走。而且當時的天氣、路況和能見度都很好，被告沒有理由說他不能注意。但是被告就是疏忽大意，沒有發現原告正要從西邊往東邊走過行人穿越道，就這樣直接開過去，撞到了原告。原告因此受傷，傷勢包括頭部外傷、頸部拉傷和挫傷、右手肘和右手挫傷、下背部挫傷、頸部扭傷等等。\n",
    "\n",
    "二、被告受傷情形：\n",
    "原告在車禍發生當天，也就是109年4月2日，先被送到臺北市立萬芳醫院急診。後來又繼續在萬芳醫院門診追蹤治療，還去了吉辰中醫診所、春林復健科診所、臺北市立聯合醫院松德院區、中山醫療社團法人中山醫院、臺北榮民總醫院、尹書田醫療財團法人書田泌尿科眼科診所等醫院復健。原告為了生活上的需要，還花錢買了一些醫療用品。這些都有診斷證明書和醫療收據可以證明。原告請求被告賠償的醫療費用和增加生活上需要的費用，總共是13萬元，這是依照民法第193條第1項的規定。\n",
    "\n",
    "三、請求賠償的事實根據：\n",
    "在車禍發生之前，原告在基邑園藝工程有限公司上班，每個月薪水是45,000元。從車禍發生後到110年1月，原告已經有9個月不能工作，薪資損失本來應該有40萬5,000元，但是原告只請求20萬元而已。而且原告因為車禍受傷，身心都受到創傷，還得了重度憂鬱症和創傷後壓力症候群。車禍的後遺症讓原告沒辦法久站或久坐，到現在都還在休養當中，沒有辦法工作。此外，原告在這次車禍受傷以後，每次走在行人穿越道上都會擔心再次被車撞飛，精神一直處在緊張不安的狀態。而且到現在傷勢都還沒完全康復，要繼續門診追蹤和復健，嚴重影響了生活作息。原告的精神上受到很大的痛苦，心理壓力也很大，加上原告還要一個人扶養兩個還在讀書的小孩，所以原告請求被告賠償50萬元的精神慰撫金。\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"你的繁體中文能力如何?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"我可以進行流暢的繁體中文對話，請儘管發問!\"},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"任務內容：從判決書的內容反推律師在撰寫起訴狀的時候的會使用的敘述。\n",
    "最下面會有判決書的內容，請你從判決書內容生成模擬律師如果要生成起訴狀的話，會有哪些必要的語句，盡量要口語化和簡單。\n",
    "我要改寫成三個大段落：\n",
    "一、 事故發生緣由\n",
    "二、 被告受傷情形\n",
    "三、 包含請求賠償的事實根據\n",
    "- 需要保留內容：重點事項要維持一樣的，包括如果原本在判決書中有條文的話，雖然不口語但是也要保留；如果是被告和原告的互動描述也要保留；受傷情形、事故發生緣由、包含請求賠償的事實根據保持原有的詳細程度；要保留相關的數字，像是住院長度，不能工作的長度，也都要保留；地址、姓名、車牌等訊息也要保留，不要自己新創內容；如果原文有引用判決的時候，必須要保留把判決的年度跟號碼寫出來。\n",
    "- 需要注意的內容：你是要模擬律師，所以主詞還是都要是'被告'或是'原告'，不要用'我'；'不要'把賠償金額加總起來或是計算，就單純保持原樣即可。\n",
    "- 內容要求：非常的口語化和簡單。\n",
    "以下為判決書內容：\"\"\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"請告訴我判決書內容以生成起訴狀\"},\n",
    "    {\"role\": \"user\", \"content\": test_prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(input_ids, max_new_tokens=4096)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL)\n",
    "# Upload the finetuned model to HuggingFace\n",
    "\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "!huggingface-cli login\n",
    "\n",
    "model.push_to_hub(\"Llamarider222/Mixtral-8x7b-Instruct-GPTQ\", check_pr=True)\n",
    "\n",
    "tokenizer.push_to_hub(\"Llamarider222/Mixtral-8x7b-Instruct-GPTQ\",check_pr=True)\n",
    "\n",
    "config.push_to_hub(\"Llamarider222/Mixtral-8x7b-Instruct-GPTQ\", check_pr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, MixtralForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "pretrained_model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "base_model = MixtralForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path,  # Mixtral, same as before\n",
    "    # quantization_config=bnb_config,  # Same quantization config as before, but commented out as its a GPTQ model (which is already quantized )\n",
    "    quantization_config=quantization_config_loading,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-finetune-alpaca-GPTQ/checkpoint-500\")\n",
    "\n",
    "# Here, \"mistral-finetune-alpaca-GPTQ/checkpoint-500\" is the adapter name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"\"Given an instruction sentence construct the output.\n",
    "\n",
    "### Instruction sentence:\n",
    "Generate a sentence that describes the main idea behind a stock market crash.\n",
    "\n",
    "\n",
    "### Output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=50)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
